# Image Edit Benchmark Pipeline - ä½¿ç”¨æŒ‡å—

## ğŸ“š ç›®å½•
1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [é…ç½®æ–‡ä»¶è¯¦è§£](#é…ç½®æ–‡ä»¶è¯¦è§£)
3. [å®ç°è‡ªå®šä¹‰æ¨¡å‹](#å®ç°è‡ªå®šä¹‰æ¨¡å‹)
4. [è¿è¡Œè¯„æµ‹](#è¿è¡Œè¯„æµ‹)
5. [æŸ¥çœ‹ç»“æœ](#æŸ¥çœ‹ç»“æœ)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd /data2/yixuan/image_edit_benchmark
pip install -r requirements.txt
```

### 2. å‡†å¤‡é…ç½®æ–‡ä»¶

```bash
# å¤åˆ¶é…ç½®æ¨¡æ¿
cp config_template.yaml config.yaml

# ç¼–è¾‘é…ç½®æ–‡ä»¶
vim config.yaml
```

### 3. é…ç½®ä½ çš„æ•°æ®å’Œæ¨¡å‹

ç¼–è¾‘ `config.yaml`ï¼Œè®¾ç½®ä»¥ä¸‹å…³é”®å‚æ•°ï¼š

- `benchmark.data_path`: ä½ çš„benchmark JSONæ–‡ä»¶è·¯å¾„
- `benchmark.categories`: äº”ä¸ªç±»åˆ«çš„åç§°
- `diffusion_model.class_path`: ä½ çš„æ‰©æ•£ç¼–è¾‘æ¨¡å‹ç±»è·¯å¾„
- `reward_model.class_path`: ä½ çš„rewardè¯„åˆ†æ¨¡å‹ç±»è·¯å¾„
- `prompts`: æ¯ä¸ªç±»åˆ«çš„è¯„åˆ†prompt

### 4. è¿è¡Œè¯„æµ‹

```bash
python main.py --config config.yaml
```

---

## é…ç½®æ–‡ä»¶è¯¦è§£

### Benchmarkæ•°æ®é…ç½®

```yaml
benchmark:
  data_path: "path/to/benchmark.json"  # å¿…å¡«
  categories:  # å¿…å¡«ï¼šäº”ä¸ªç±»åˆ«åç§°
    - "category_1"
    - "category_2"
    - "category_3"
    - "category_4"
    - "category_5"
```

### æ‰©æ•£æ¨¡å‹é…ç½®

```yaml
diffusion_model:
  # æ¨¡å‹ç±»çš„å®Œæ•´è·¯å¾„ï¼ˆæ¨¡å—.ç±»åï¼‰
  class_path: "src.models.diffusion.implementations.example_model.ExampleDiffusionModel"
  
  params:  # ä¼ é€’ç»™æ¨¡å‹çš„å‚æ•°
    model_name: "timbrooks/instruct-pix2pix"
    device: "cuda"
    batch_size: 1
    num_inference_steps: 50
    guidance_scale: 7.5
```

### Rewardæ¨¡å‹é…ç½®

```yaml
reward_model:
  class_path: "src.models.reward.implementations.example_reward.ExampleRewardModel"
  
  params:
    model_name: "your-vlm-model"
    device: "cuda"
    temperature: 0.7
```

### Prompté…ç½®

```yaml
prompts:
  category_1:  # æ¯ä¸ªç±»åˆ«éƒ½éœ€è¦é…ç½®
    system_prompt: "You are an expert image quality evaluator."
    user_prompt_template: |
      Original description: {original_description}
      Edit instruction: {edit_instruction}
      Please rate the edited image quality on a scale of 0-10.
```

**æ³¨æ„**ï¼š`user_prompt_template` ä¸­å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å˜é‡ï¼š
- `{original_description}`: åŸå›¾æè¿°
- `{edit_instruction}`: ç¼–è¾‘æŒ‡ä»¤

---

## å®ç°è‡ªå®šä¹‰æ¨¡å‹

### æ­¥éª¤1: åˆ›å»ºè‡ªå®šä¹‰æ‰©æ•£æ¨¡å‹

åœ¨ `src/models/diffusion/implementations/` ç›®å½•ä¸‹åˆ›å»ºæ–°æ–‡ä»¶ï¼Œä¾‹å¦‚ `my_model.py`ï¼š

```python
from PIL import Image
from ..base_diffusion import BaseDiffusionModel

class MyDiffusionModel(BaseDiffusionModel):
    def _initialize(self):
        """åˆå§‹åŒ–ä½ çš„æ¨¡å‹"""
        self.model_name = self.config.get("model_name")
        # åŠ è½½æ¨¡å‹...
        
    def edit_image(self, original_image: Image.Image, 
                   edit_instruction: str, **kwargs) -> Image.Image:
        """å®ç°å›¾åƒç¼–è¾‘é€»è¾‘"""
        # ä½ çš„ç¼–è¾‘é€»è¾‘...
        return edited_image
```

### æ­¥éª¤2: åˆ›å»ºè‡ªå®šä¹‰Rewardæ¨¡å‹

åœ¨ `src/models/reward/implementations/` ç›®å½•ä¸‹åˆ›å»ºæ–°æ–‡ä»¶ï¼Œä¾‹å¦‚ `my_reward.py`ï¼š

```python
from PIL import Image
from typing import Optional
from ..base_reward import BaseRewardModel

class MyRewardModel(BaseRewardModel):
    def _initialize(self):
        """åˆå§‹åŒ–ä½ çš„rewardæ¨¡å‹"""
        # åŠ è½½æ¨¡å‹...
        
    def score(self, edited_image: Image.Image,
              original_description: str,
              edit_instruction: str,
              system_prompt: str,
              user_prompt: str,
              original_image: Optional[Image.Image] = None,
              **kwargs) -> float:
        """å®ç°è¯„åˆ†é€»è¾‘"""
        # ä½ çš„è¯„åˆ†é€»è¾‘...
        return score  # è¿”å›0-10çš„åˆ†æ•°
```

### æ­¥éª¤3: åœ¨é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨

```yaml
diffusion_model:
  class_path: "src.models.diffusion.implementations.my_model.MyDiffusionModel"
  params:
    model_name: "your-model-path"

reward_model:
  class_path: "src.models.reward.implementations.my_reward.MyRewardModel"
  params:
    model_name: "your-reward-model-path"
```

---

## è¿è¡Œè¯„æµ‹

### åŸºæœ¬è¿è¡Œ

```bash
python main.py --config config.yaml
```

### ä»æ–­ç‚¹ç»§ç»­

å¦‚æœè¯„æµ‹ä¸­æ–­ï¼Œå¯ä»¥ä»æ–­ç‚¹ç»§ç»­ï¼š

```bash
python main.py --config config.yaml --resume
```

### æŸ¥çœ‹å¸®åŠ©

```bash
python main.py --help
```

---

## æŸ¥çœ‹ç»“æœ

è¯„æµ‹å®Œæˆåï¼Œç»“æœä¼šä¿å­˜åœ¨ `outputs/results/` ç›®å½•ï¼š

### JSONæŠ¥å‘Š

```bash
cat outputs/results/evaluation_report_YYYYMMDD_HHMMSS.json
```

åŒ…å«å®Œæ•´çš„ç»Ÿè®¡æ•°æ®ï¼Œé€‚åˆç¨‹åºå¤„ç†ã€‚

### MarkdownæŠ¥å‘Š

```bash
cat outputs/results/evaluation_report_YYYYMMDD_HHMMSS.md
```

äººç±»å¯è¯»çš„æ ¼å¼ï¼ŒåŒ…å«ï¼š
- æ€»ä½“æ‘˜è¦
- å„ç±»åˆ«è¯¦ç»†ç»Ÿè®¡
- æœ€å¥½/æœ€å·®ç±»åˆ«

### ç”Ÿæˆçš„å›¾åƒ

å¦‚æœåœ¨é…ç½®ä¸­å¯ç”¨äº† `save_generated_images: true`ï¼Œç¼–è¾‘åçš„å›¾åƒä¼šä¿å­˜åœ¨ï¼š

```
outputs/images/
â”œâ”€â”€ category_1/
â”‚   â”œâ”€â”€ pair_001.png
â”‚   â””â”€â”€ pair_002.png
â”œâ”€â”€ category_2/
â”‚   â””â”€â”€ ...
```

---

## å¸¸è§é—®é¢˜

### Q1: æˆ‘çš„benchmark JSONæ ¼å¼ä¸åŒæ€ä¹ˆåŠï¼Ÿ

**A**: æœ‰ä¸¤ç§æ–¹æ³•ï¼š

1. **è°ƒæ•´æ•°æ®æ ¼å¼**ï¼šä¿®æ”¹JSONä½¿å…¶ç¬¦åˆé»˜è®¤æ ¼å¼
2. **è‡ªå®šä¹‰loader**ï¼šä¿®æ”¹ `src/data/benchmark_loader.py` ä¸­çš„ `_extract_category_data` æ–¹æ³•

é»˜è®¤æ ¼å¼ï¼š
```json
{
  "category_name": [
    {
      "id": "xxx",
      "original_image_b64": "...",
      "edit_instruction": "...",
      "original_description": "..."
    }
  ]
}
```

### Q2: å¦‚ä½•ä½¿ç”¨ä¸åŒçš„è¯„åˆ†æ ‡å‡†ï¼Ÿ

**A**: åœ¨é…ç½®æ–‡ä»¶çš„ `prompts` éƒ¨åˆ†ä¸ºæ¯ä¸ªç±»åˆ«è‡ªå®šä¹‰promptï¼š

```yaml
prompts:
  object_addition:
    system_prompt: "Focus on whether the object was added correctly."
    user_prompt_template: |
      Instruction: {edit_instruction}
      Rate how well the object was added (0-10).
```

### Q3: è¯„æµ‹é€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: å¯ä»¥ï¼š

1. **å‡å°‘inference steps**ï¼šåœ¨æ¨¡å‹é…ç½®ä¸­é™ä½ `num_inference_steps`
2. **å¯ç”¨æ‰¹å¤„ç†**ï¼šå®ç°æ¨¡å‹çš„ `batch_edit` å’Œ `batch_score` æ–¹æ³•
3. **ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹**ï¼šé€‰æ‹©æ›´è½»é‡çš„æ‰©æ•£æ¨¡å‹

### Q4: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**A**: 

1. **ä¸ä¿å­˜ç”Ÿæˆå›¾åƒ**ï¼šè®¾ç½® `save_generated_images: false`
2. **ä½¿ç”¨æ›´å°çš„batch size**ï¼šå‡å° `batch_size`
3. **ä½¿ç”¨float16**ï¼šåœ¨æ¨¡å‹ä¸­ä½¿ç”¨ `torch.float16`

### Q5: å¦‚ä½•æµ‹è¯•å•ä¸ªæ ·æœ¬ï¼Ÿ

**A**: ä½¿ç”¨Pipelineçš„ `run_single_pair` æ–¹æ³•ï¼š

```python
from src.pipeline import BenchmarkPipeline
import yaml

with open('config.yaml') as f:
    config = yaml.safe_load(f)

pipeline = BenchmarkPipeline(config)

result = pipeline.run_single_pair(
    original_image_b64="your_base64_string",
    edit_instruction="make it blue",
    original_description="a red car",
    category="category_1"
)

print(f"Score: {result['score']}")
```

---

## é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰è¯„åˆ†æŒ‡æ ‡

åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šéœ€è¦è®¡ç®—çš„ç»Ÿè®¡æŒ‡æ ‡ï¼š

```yaml
evaluation:
  metrics:
    - "mean"      # å¹³å‡å€¼
    - "std"       # æ ‡å‡†å·®
    - "median"    # ä¸­ä½æ•°
    - "min"       # æœ€å°å€¼
    - "max"       # æœ€å¤§å€¼
```

### è‡ªå®šä¹‰æ—¥å¿—çº§åˆ«

```yaml
logging:
  level: "DEBUG"  # DEBUG, INFO, WARNING, ERROR
  console_output: true
  file_output: true
  log_file: "outputs/logs/evaluation.log"
```

---

## ç¤ºä¾‹ä»£ç 

æŸ¥çœ‹ `examples/` ç›®å½•è·å–æ›´å¤šç¤ºä¾‹ï¼š

- `run_evaluation.py`: å®Œæ•´è¯„æµ‹ç¤ºä¾‹
- `custom_model_example.py`: è‡ªå®šä¹‰æ¨¡å‹ç¤ºä¾‹

---

## è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ï¼š`outputs/logs/evaluation.log`
2. æ£€æŸ¥é…ç½®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®
3. ç¡®è®¤æ¨¡å‹ç±»è·¯å¾„æ˜¯å¦æ­£ç¡®
4. æŸ¥çœ‹ç¤ºä¾‹ä»£ç 

---

ç¥è¯„æµ‹é¡ºåˆ©ï¼ğŸ‰


