# ğŸš€ å¤šGPUè¯„åˆ†è§£å†³æ–¹æ¡ˆ

## é—®é¢˜èƒŒæ™¯

### è§‚å¯Ÿåˆ°çš„ç°è±¡
ç”¨æˆ·åœ¨è¯„åˆ†é˜¶æ®µå‘ç°ï¼š
- âœ… GPU 0ï¼š63% åˆ©ç”¨ç‡ï¼Œæ­£å¸¸å·¥ä½œ
- âŒ GPU 1-5ï¼š0% åˆ©ç”¨ç‡ï¼Œå®Œå…¨ç©ºé—²

### åŸå› åˆ†æ

#### 1. Qwen3-VL-30B å•å¡èƒ½è£…ä¸‹
- **æ¨¡å‹å¤§å°**ï¼š30Bå‚æ•° Ã— 2å­—èŠ‚(bfloat16) â‰ˆ **60GB**
- **H100æ˜¾å­˜**ï¼š80GB
- **ç»“è®º**ï¼šå•å¡è¶³å¤Ÿï¼Œæ— éœ€æ¨¡å‹å¹¶è¡Œ

#### 2. `device_map="auto"` çš„é»˜è®¤è¡Œä¸º
```python
model = AutoModelForImageTextToText.from_pretrained(
    model_name,
    device_map="auto"  # å¦‚æœå•å¡èƒ½è£…ä¸‹ï¼Œå°±åªç”¨å•å¡
)
```
- transformers çš„ `device_map="auto"` ä¼šå°è¯•æœ€ä¼˜ç­–ç•¥
- å¦‚æœæ¨¡å‹èƒ½è£…è¿›å•å¡ï¼Œå°±**ä¼˜å…ˆä½¿ç”¨å•å¡**ï¼ˆå‡å°‘GPUé—´é€šä¿¡å¼€é”€ï¼‰
- åªæœ‰æ¨¡å‹å¤ªå¤§ï¼ˆè¶…è¿‡å•å¡æ˜¾å­˜ï¼‰æ—¶ï¼Œæ‰ä¼šè‡ªåŠ¨è¿›è¡Œæ¨¡å‹å¹¶è¡Œ

#### 3. Batch Inference â‰  å¤šGPU
- **Batch Inference**ï¼šåœ¨**å•ä¸ªæ¨¡å‹**ä¸ŠåŒæ—¶å¤„ç†å¤šä¸ªæ ·æœ¬
- **å¤šGPUåŠ é€Ÿ**ï¼šéœ€è¦é¢å¤–é…ç½®ï¼ˆæ¨¡å‹å¹¶è¡Œæˆ–æ•°æ®å¹¶è¡Œï¼‰

---

## è§£å†³æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆAï¼šæ•°æ®å¹¶è¡Œï¼ˆæ¨è â­ï¼‰

**åŸç†**ï¼šæ¯ä¸ªGPUè¿è¡Œä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹ï¼Œå¤„ç†ä¸åŒçš„å›¾åƒ

```
GPU 0: æ¨¡å‹A â†’ å¤„ç† images 0, 6, 12, 18, ...
GPU 1: æ¨¡å‹B â†’ å¤„ç† images 1, 7, 13, 19, ...
GPU 2: æ¨¡å‹C â†’ å¤„ç† images 2, 8, 14, 20, ...
GPU 3: æ¨¡å‹D â†’ å¤„ç† images 3, 9, 15, 21, ...
GPU 4: æ¨¡å‹E â†’ å¤„ç† images 4, 10, 16, 22, ...
GPU 5: æ¨¡å‹F â†’ å¤„ç† images 5, 11, 17, 23, ...
```

**ä¼˜ç‚¹**ï¼š
- âœ… **é€Ÿåº¦æœ€å¿«**ï¼ˆç†è®ºä¸Š 6å€åŠ é€Ÿï¼‰
- âœ… æ— GPUé—´é€šä¿¡å¼€é”€
- âœ… å®ç°ç®€å•

**ç¼ºç‚¹**ï¼š
- âŒ æ˜¾å­˜å ç”¨ = 6 Ã— 60GB = 360GBï¼ˆä½†æ‚¨æœ‰ 6Ã—80GB = 480GBï¼Œç»°ç»°æœ‰ä½™ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… å¤šä¸ªGPUå¯ç”¨
- âœ… æ¯ä¸ªGPUæ˜¾å­˜è¶³å¤Ÿè£…ä¸‹å®Œæ•´æ¨¡å‹
- âœ… éœ€è¦å¤„ç†å¤§é‡å›¾åƒ

---

### æ–¹æ¡ˆBï¼šå¼ºåˆ¶æ¨¡å‹å¹¶è¡Œï¼ˆä¸æ¨èï¼‰

**åŸç†**ï¼šå°†ä¸€ä¸ªæ¨¡å‹çš„ä¸åŒå±‚åˆ†é…åˆ°ä¸åŒGPU

```
GPU 0: Layers 0-10  â†˜
GPU 1: Layers 11-20  â†’ å•ä¸ªå›¾åƒä¾æ¬¡ç»è¿‡æ‰€æœ‰GPU
GPU 2: Layers 21-30  â†—
...
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¯ä»¥å¤„ç†è¶…å¤§æ¨¡å‹ï¼ˆå•å¡è£…ä¸ä¸‹ï¼‰

**ç¼ºç‚¹**ï¼š
- âŒ GPUé—´é€šä¿¡å¼€é”€å¤§
- âŒ é€Ÿåº¦æ…¢ï¼ˆéœ€è¦ç­‰å¾…å±‚é—´ä¼ è¾“ï¼‰
- âŒ å¯¹äº30Bè¿™ç§å•å¡èƒ½è£…ä¸‹çš„æ¨¡å‹ï¼Œåè€Œå˜æ…¢

**é€‚ç”¨åœºæ™¯**ï¼š
- âŒ ä¸é€‚åˆæ‚¨çš„æƒ…å†µï¼ˆ30Bå•å¡èƒ½è£…ä¸‹ï¼‰

---

## å®ç°ç»†èŠ‚ï¼ˆæ–¹æ¡ˆAï¼šæ•°æ®å¹¶è¡Œï¼‰

### 1. æ ¸å¿ƒå®ç°

**æ–°å¢æ–‡ä»¶**ï¼š`src/models/reward/implementations/qwen3_vl_multi_gpu_subprocess.py`

**å…³é”®ä»£ç **ï¼š
```python
class Qwen3VLMultiGPUSubprocessRewardModel(BaseRewardModel):
    def __init__(self, config):
        # å¤šGPUé…ç½®
        self.device_ids = config.get("device_ids", [0, 1, 2, 3, 4, 5])
        self.num_gpus = len(self.device_ids)
        
    def batch_score(self, edited_images, ...):
        # 1. å°†ä»»åŠ¡åˆ†é…åˆ°å„ä¸ªGPU
        gpu_tasks = [[] for _ in range(self.num_gpus)]
        for i, task in enumerate(all_tasks):
            gpu_idx = i % self.num_gpus
            gpu_tasks[gpu_idx].append(task)
        
        # 2. å¹¶è¡Œæ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.num_gpus) as executor:
            futures = []
            for gpu_idx, gpu_id in enumerate(self.device_ids):
                future = executor.submit(
                    self._call_subprocess_single_gpu,
                    gpu_tasks[gpu_idx],
                    gpu_id  # æŒ‡å®šä½¿ç”¨å“ªä¸ªGPU
                )
                futures.append(future)
            
            # 3. æ”¶é›†ç»“æœ
            for future in futures:
                scores = future.result()
                all_scores.extend(scores)
        
        return all_scores
    
    def _call_subprocess_single_gpu(self, tasks, gpu_id):
        """åœ¨æŒ‡å®šGPUä¸Šè¿è¡Œç‹¬ç«‹çš„è¯„åˆ†è¿›ç¨‹"""
        cmd = [
            'conda', 'run', '-n', 'yx_qwen3',
            'python', 'qwen3_vl_standalone.py',
            '--device', f'cuda:{gpu_id}',  # â­ æŒ‡å®šGPU
            ...
        ]
        subprocess.run(cmd)
```

### 2. ä»»åŠ¡åˆ†é…ç¤ºä¾‹

å‡è®¾æœ‰ 10 å¼ å›¾åƒï¼Œ6 ä¸ªGPUï¼š

```
GPU 0: Task 0, Task 6       â†’ 2 tasks
GPU 1: Task 1, Task 7       â†’ 2 tasks
GPU 2: Task 2, Task 8       â†’ 2 tasks
GPU 3: Task 3, Task 9       â†’ 2 tasks
GPU 4: Task 4               â†’ 1 task
GPU 5: Task 5               â†’ 1 task
```

### 3. æ˜¾å­˜ä½¿ç”¨

**å•GPUæ¨¡å¼**ï¼ˆå½“å‰ï¼‰ï¼š
- GPU 0: 60GB (æ¨¡å‹) + 5GB (batch=4) = 65GB
- GPU 1-5: 0GB

**å¤šGPUæ¨¡å¼**ï¼ˆæ–¹æ¡ˆAï¼‰ï¼š
- GPU 0: 60GB (æ¨¡å‹) + 2GB (batch=2) = 62GB
- GPU 1: 60GB (æ¨¡å‹) + 2GB (batch=2) = 62GB
- GPU 2: 60GB (æ¨¡å‹) + 2GB (batch=2) = 62GB
- GPU 3: 60GB (æ¨¡å‹) + 2GB (batch=2) = 62GB
- GPU 4: 60GB (æ¨¡å‹) + 2GB (batch=2) = 62GB
- GPU 5: 60GB (æ¨¡å‹) + 2GB (batch=2) = 62GB

**æ€»æ˜¾å­˜**ï¼š372GB / 480GB (77.5%)

---

## ä½¿ç”¨æ–¹æ³•

### é…ç½®æ–‡ä»¶

**æ–°é…ç½®**ï¼š`config_full_multi_gpu.yaml`

```yaml
reward_model:
  type: "qwen3_vl_multi_gpu_subprocess"  # â­ ä½¿ç”¨å¤šGPUç‰ˆæœ¬
  class_path: "src.models.reward.implementations.qwen3_vl_multi_gpu_subprocess.Qwen3VLMultiGPUSubprocessRewardModel"
  params:
    model_name: "path/to/Qwen3-VL-30B"
    device_ids: [0, 1, 2, 3, 4, 5]  # â­ ä½¿ç”¨6ä¸ªGPU
    dtype: "bfloat16"
    batch_size: 2  # æ¯ä¸ªGPUçš„batch sizeï¼ˆæ€»batch=6Ã—2=12ï¼‰
    conda_env: "yx_qwen3"
```

### è¿è¡Œå‘½ä»¤

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate yx_grpo_rl_post_edit

# è¿è¡Œï¼ˆä½¿ç”¨æ–°é…ç½®ï¼‰
cd /data2/yixuan/image_edit_benchmark
python main.py --config config_full_multi_gpu.yaml
```

---

## æ€§èƒ½å¯¹æ¯”

### å•GPUæ¨¡å¼ï¼ˆå½“å‰ï¼‰

| é˜¶æ®µ | GPUä½¿ç”¨ | æ—¶é—´ |
|------|---------|------|
| ç¼–è¾‘ | 6ä¸ªGPUå¹¶è¡Œ | ~3åˆ†é’Ÿ (10å¼ å›¾) |
| è¯„åˆ† | 1ä¸ªGPU | ~4åˆ†é’Ÿ (10å¼ å›¾) |
| **æ€»è®¡** | - | **~7åˆ†é’Ÿ** |

### å¤šGPUæ¨¡å¼ï¼ˆæ–¹æ¡ˆAï¼‰

| é˜¶æ®µ | GPUä½¿ç”¨ | æ—¶é—´ |
|------|---------|------|
| ç¼–è¾‘ | 6ä¸ªGPUå¹¶è¡Œ | ~3åˆ†é’Ÿ (10å¼ å›¾) |
| è¯„åˆ† | **6ä¸ªGPUå¹¶è¡Œ** | **~40ç§’** (10å¼ å›¾) |
| **æ€»è®¡** | - | **~3åˆ†40ç§’** |

**åŠ é€Ÿæ¯”**ï¼š
- è¯„åˆ†é˜¶æ®µï¼š**6å€åŠ é€Ÿ**
- æ€»ä½“ï¼š**1.9å€åŠ é€Ÿ**

### å®Œæ•´Benchmarkï¼ˆ900å¯¹ï¼‰

| æ¨¡å¼ | ç¼–è¾‘æ—¶é—´ | è¯„åˆ†æ—¶é—´ | æ€»æ—¶é—´ |
|------|----------|----------|--------|
| å•GPUè¯„åˆ† | ~4.5å°æ—¶ | **~6å°æ—¶** | **~10.5å°æ—¶** |
| å¤šGPUè¯„åˆ† | ~4.5å°æ—¶ | **~1å°æ—¶** | **~5.5å°æ—¶** |

**èŠ‚çœæ—¶é—´**ï¼š**5å°æ—¶** â°

---

## å®æ—¶æ•ˆæœ

### ç¼–è¾‘é˜¶æ®µï¼ˆå·²æœ‰ï¼‰
```
[GPU 0] Denoising: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:17<00:00]
[GPU 1] Denoising:  87%|â–ˆâ–ˆâ–ˆ | 26/30 [00:15<00:02]
[GPU 2] Denoising:  93%|â–ˆâ–ˆâ–ˆâ–ˆ| 28/30 [00:16<00:01]
...
[SYNC] Editing images: 100%|â–ˆ| 10/10 [02:53<00:00]
```

### è¯„åˆ†é˜¶æ®µï¼ˆæ–°å¢ï¼‰
```
[GPU 0] [Qwen3-VL Scoring] Starting batch scoring for 2 images
[GPU 0]   [Sample   0] Score: 8.50 | Response: The image successfully...
[GPU 0]   [Sample   1] Score: 7.20 | Response: The image effectively...
[GPU 0] [Batch 1] Images 0-1 done, avg_score=7.850

[GPU 1] [Qwen3-VL Scoring] Starting batch scoring for 2 images
[GPU 1]   [Sample   0] Score: 9.10 | Response: The metamorphosis...
[GPU 1]   [Sample   1] Score: 6.80 | Response: The image shows...
[GPU 1] [Batch 1] Images 0-1 done, avg_score=7.950

... (GPU 2-5 åŒæ—¶è¾“å‡º)

Multi-GPU scoring completed!
```

---

## æŠ€æœ¯è¦ç‚¹

### 1. ç‹¬ç«‹è¿›ç¨‹éš”ç¦»
- æ¯ä¸ªGPUè¿è¡Œ**ç‹¬ç«‹çš„Pythonè¿›ç¨‹**
- å„è¿›ç¨‹ç›¸äº’ç‹¬ç«‹ï¼Œæ— èµ„æºç«äº‰
- ä½¿ç”¨ subprocess + conda run å®ç°ç¯å¢ƒéš”ç¦»

### 2. è½®è¯¢ä»»åŠ¡åˆ†é…
```python
for i, task in enumerate(tasks):
    gpu_idx = i % num_gpus  # è½®è¯¢åˆ†é…
    gpu_tasks[gpu_idx].append(task)
```

### 3. çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
```python
with ThreadPoolExecutor(max_workers=num_gpus) as executor:
    futures = [
        executor.submit(worker, tasks, gpu_id)
        for gpu_id, tasks in zip(device_ids, gpu_tasks)
    ]
```

---

## æ³¨æ„äº‹é¡¹

### 1. æ˜¾å­˜è¦æ±‚
- æ¯ä¸ªGPUéœ€è¦ ~62GB æ˜¾å­˜
- ç¡®ä¿æ‰€æœ‰GPUéƒ½æ˜¯H100 80GB

### 2. ç¯å¢ƒè¦æ±‚
- ç¡®ä¿ `yx_qwen3` ç¯å¢ƒå·²æ­£ç¡®é…ç½®
- è¿è¡Œ `./setup_qwen3_vl_env.sh` è‡ªåŠ¨è®¾ç½®

### 3. æ‰¹æ¬¡å¤§å°è°ƒæ•´
```yaml
# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå‡å°æ¯ä¸ªGPUçš„batch size
batch_size: 2  # æ¯ä¸ªGPU: 2å¼ å›¾
# æ€»throughput = 6 GPUs Ã— 2 images = 12 images/batch
```

---

## å¯¹æ¯”æ€»ç»“

| ç‰¹æ€§ | å•GPUæ¨¡å¼ | å¤šGPUæ¨¡å¼ (æ–¹æ¡ˆA) |
|------|----------|-------------------|
| **GPUåˆ©ç”¨ç‡** | GPU 0: 63%, å…¶ä»–: 0% | æ‰€æœ‰GPU: ~60% |
| **è¯„åˆ†é€Ÿåº¦** | æ…¢ | **å¿« 6å€** âš¡ |
| **æ˜¾å­˜ä½¿ç”¨** | 65GB (ä»…GPU 0) | 372GB (åˆ†å¸ƒåœ¨6ä¸ªGPU) |
| **å®ç°å¤æ‚åº¦** | ç®€å• | ä¸­ç­‰ |
| **é€‚ç”¨åœºæ™¯** | å•GPUæˆ–å°æ•°æ®é›† | **å¤šGPU + å¤§æ•°æ®é›†** â­ |

---

## å¿«é€Ÿå¼€å§‹

```bash
# 1. ç¡®ä¿ç¯å¢ƒå·²é…ç½®
cd /data2/yixuan/image_edit_benchmark
./setup_qwen3_vl_env.sh

# 2. è¿è¡Œå¤šGPUè¯„åˆ†
conda activate yx_grpo_rl_post_edit
python main.py --config config_full_multi_gpu.yaml

# 3. ç›‘æ§GPUä½¿ç”¨
watch -n 1 nvidia-smi
```

---

## æ€»ç»“

### å½“å‰é—®é¢˜
- âœ… **ä¸æ˜¯bug**ï¼šè¿™æ˜¯ `device_map="auto"` çš„é¢„æœŸè¡Œä¸º
- âœ… 30Bæ¨¡å‹å•å¡èƒ½è£…ä¸‹ï¼Œæ‰€ä»¥åªç”¨äº†GPU 0

### è§£å†³æ–¹æ¡ˆ
- â­ **æ–¹æ¡ˆAï¼ˆæ¨èï¼‰**ï¼šæ•°æ®å¹¶è¡Œï¼Œ6ä¸ªGPUå„è¿è¡Œç‹¬ç«‹æ¨¡å‹
  - **é€Ÿåº¦**ï¼šè¯„åˆ†é˜¶æ®µ 6å€åŠ é€Ÿ
  - **æ˜¾å­˜**ï¼š372GB / 480GB
  - **å®ç°**ï¼šå·²å®Œæˆï¼Œä½¿ç”¨ `config_full_multi_gpu.yaml`

### æ•ˆæœ
- ğŸš€ è¯„åˆ†é˜¶æ®µä» ~6å°æ—¶ â†’ ~1å°æ—¶
- ğŸ¯ å®Œæ•´benchmarkä» ~10.5å°æ—¶ â†’ ~5.5å°æ—¶
- ğŸ’° èŠ‚çœ **5å°æ—¶** è®¡ç®—æ—¶é—´

**å»ºè®®ä½¿ç”¨æ–¹æ¡ˆAï¼Œå……åˆ†åˆ©ç”¨æ‚¨çš„6å¼ H100ï¼** ğŸ‰

